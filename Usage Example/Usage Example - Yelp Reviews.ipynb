{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Academic dataset\n",
    "\n",
    "Yelp released an [academic dataset](https://www.yelp.com/dataset/challenge) to engage ML researchers in solving their problems, particularly those related to image processing and natural language processing (NLP).\n",
    "\n",
    "We'll do an intro to NLP methods using some functions I wrote that simplifies some of the processes involved. I use mostly Spacy and Gensim here, but here are some libraries that I've found helpful in other applications:\n",
    "\n",
    "1. [Spacy](https://spacy.io/) - tokenizing, tagging, part of speech, entity recognition, etc.\n",
    "2. [Gensim](https://radimrehurek.com/gensim/) - topic modeling, phrase modeling, etc.\n",
    "3. [NLTK](https://www.nltk.org/) - tokenizing, tagging, part of speech, entity recognition, etc.\n",
    "4. [Pattern](https://www.clips.uantwerpen.be/pattern) - tokenizing, tagging, part of speech, entity recognition, etc.\n",
    "5. [PyLdaVis](https://github.com/bmabey/pyLDAvis) - visualize LDA topics (port from R)\n",
    "4. [wordcloud](https://github.com/amueller/word_cloud) - creating word clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, pandas as pd, numpy as np, os, random, ujson, codecs\n",
    "scriptpath = \"../Code/\"\n",
    "sys.path.append(scriptpath)\n",
    "import spacy, re, gensim, numpy as np, pandas as pd, json, codecs, pyLDAvis, warnings\n",
    "from gensim.models import Phrases, Word2Vec\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import basic_text_processing_functions as tx\n",
    "floc = 'C:/Users/yangy/Documents/Data/Yelp/yelp_dataset_challenge_round9/yelp_dataset_challenge_round9/' # Download the dataset locally and change the floc to the correct place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._Dataset_Challenge_Dataset_Agreement.pdf',\n",
       " '._Yelp_Dataset_Challenge_Terms_round_9.pdf',\n",
       " 'Dataset_Challenge_Dataset_Agreement.pdf',\n",
       " 'PaxHeader',\n",
       " 'texts_only.txt',\n",
       " 'yelp_academic_dataset_business.json',\n",
       " 'yelp_academic_dataset_checkin.json',\n",
       " 'yelp_academic_dataset_review.json',\n",
       " 'yelp_academic_dataset_tip.json',\n",
       " 'yelp_academic_dataset_user.json',\n",
       " 'Yelp_Dataset_Challenge_Terms_round_9.pdf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = os.listdir(floc) # these are the files that the dataset comes with (there's another folder for photos)\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'yelp_academic_dataset_review.json' # reviews data\n",
    "# data = pd.read_json(floc+fname,  lines=True, encoding = 'utf-8') # load json data using ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(floc+fname,'r' ,encoding = 'utf-8') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'business_id': u'2aFiy99vNLklCx3T_tGS9A',\n",
       " u'cool': 0,\n",
       " u'date': u'2011-10-10',\n",
       " u'funny': 0,\n",
       " u'review_id': u'NxL8SIC5yqOdnlXCg18IBg',\n",
       " u'stars': 5,\n",
       " u'text': u\"If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\",\n",
       " u'type': u'review',\n",
       " u'useful': 0,\n",
       " u'user_id': u'KpkOkG6RIf4Ra25Lhhxf1A'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ujson\n",
    "d = ujson.loads(data[0]) # each line is one review\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'funny',\n",
       " u'user_id',\n",
       " u'review_id',\n",
       " u'text',\n",
       " u'business_id',\n",
       " u'stars',\n",
       " u'date',\n",
       " u'useful',\n",
       " u'type',\n",
       " u'cool']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys() # These are the variables included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we really only care about the text field. Let's write this off into a separet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"The kind of breakfast you should expect from a cheap hotel buffet. Powdered eggs and crepes with a side of tasty taters. Plus the coffee tastes like... Well it's tasteless.\\nService reminds me of Canadian Tire.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "ujson.loads(random.choice(data))['text'] # this is how we would get a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    with codecs.open(floc+'texts_only.txt','w' ,encoding = 'utf-8') as f:\n",
    "        for d in data:\n",
    "            dat = ujson.loads(d)['text'].strip().replace(u'\\n', u' ')\n",
    "            if len(dat)>1:\n",
    "                f.write(dat+u'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4156042\n"
     ]
    }
   ],
   "source": [
    "fname = 'texts_only.txt'\n",
    "with codecs.open(floc+fname,'r' ,encoding = 'utf-8') as f:\n",
    "    data = f.readlines()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Stopped by today to pick up a pint of vegan ice cream for the house and their special was Vegan Chocolate Ice Cream w/ bits of choco chip cookie & peanut butter - DIVINE!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(data).strip() # look at a random line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, normalize these texts\n",
    "\n",
    "In written language, we have grammar rules, normal sentence structures, punctuations, etc... but even when the writng is no good like this bad sentence be still meaning understand.\n",
    "\n",
    "This is because the important words remain. Much of the text analysis tools use this idea of \"bag of words\" to extract meaning from texts.\n",
    "\n",
    "Before we do this, we can convert words to normalized forms, i.e. infinitive forms of verbs: Am,is,were,are,was$\\rightarrow$be and singular forms of nouns: drinks$\\rightarrow$drink.\n",
    "\n",
    "There are several packages to deal with this: NLTK, Pattern, Textblob (uses NLTK/Pattern), Spacy.\n",
    "\n",
    "I wrote my code to work with Spacy 1.9, after upgrading to 2.x, it seems to be a lot slower. There's a current open issue for this on Spacy's [GitHub page](https://github.com/explosion/spaCy/issues/1508)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save my current configuration\n",
    "# write config file for the text processing functions\n",
    "path = 'text_analysis/'\n",
    "pathloc = floc + path\n",
    "if 1==0:\n",
    "    with codecs.open(floc+path+'default.cfg', 'w', encoding = 'utf-8') as f:\n",
    "        f.write(json.dumps({'batch_size':1000, 'n_threads':8,\n",
    "                            'fpathroot':floc+path, 'fpathappend':u'', 'entity_sub':True}))\n",
    "else:\n",
    "    tx.batch_size, tx.n_threads, tx.fpathroot, tx.fpathappend, tx.entity_sub, tx.numtopics=tx._config_text_analysis_(floc+path+'default.cfg')\n",
    "#     tx.batch_size, tx.n_threads, tx.fpathroot, tx.fpathappend, tx.entity_sub, tx.numtopics = batch_size, n_threads, fpathroot, fpathappend, entity_sub, numtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step takes a while, needs af fix on spacy's end, going back to v1.9 or Pattern/NLTK\n",
    "if 1 == 0:\n",
    "    tx._write_unigram_(floc+fname,\n",
    "                       unigram_sentences_filepath=tx.fpathroot+tx.fpathappend+'_sent_gram_0.txt',\n",
    "                       entity_sub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, create phrase models (colocation detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yangy\\AppData\\Local\\conda\\conda\\envs\\py27\\lib\\site-packages\\gensim\\models\\phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "passes = 2\n",
    "if 1==1:\n",
    "    ngrams = tx._phrase_detection_(fpath=tx.fpathroot+tx.fpathappend, passes = passes, returnmodels = True, threshold = 3.)\n",
    "else:\n",
    "    ngrams = list()\n",
    "    gramfiles = os.listdir(tx.fpathroot)\n",
    "    phrasemodels = [tx.fpathroot+g for g in gramfiles if 'phrase_model' in g]\n",
    "    for m in phrasemodels:\n",
    "        ngrams.append(Phrases.load(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typically, we want to apply parser/phrase models to doc level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regram = 0 # change to 1 to organize bags of words by documents (vs. sentences)\n",
    "# passes = 2\n",
    "if regram == 1:\n",
    "    if 1==1:\n",
    "        grammed_reviews = tx._phrase_prediction_(tx.fpathroot+fname, ngrams, outfpath = tx.fpathroot+tx.fpathappend+'grammed_texts.txt', entity_sub = True)\n",
    "    else:\n",
    "        grammed_reviews = tx.fpathroot+'grammed_texts.txt'\n",
    "else:\n",
    "    grammed_reviews = tx.fpathroot+'sent_gram_{}.txt'.format(passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary by filtering out common/uncommon tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "vocab, gensim_dictionary, cts = tx._make_dict_(grammed_reviews,\n",
    "                                          floc = tx.fpathroot+tx.fpathappend+'dict_gram.dict',\n",
    "                                          topfilter = 95,bottomfilter = 10,\n",
    "                                          no_filters=False, keep_ent=False, \n",
    "                                         discard_list={},\n",
    "                                         keep_list = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179185"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create serialized representation of corpus (token id's instead of tokens)\n",
    "\n",
    "This step in th eprocess also filters out vocab not in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1: \n",
    "    grammed_corpus = tx._serialize_corpus_(grammed_reviews, gensim_dictionary,  outfpath = tx.fpathroot+tx.fpathappend+'_serialized.mm')\n",
    "else:\n",
    "    grammed_corpus_loc= tx.fpathroot+tx.fpathappend+'_serialized.mm'\n",
    "    grammed_corpus = MmCorpus(grammed_corpus_loc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some kind of topic modeling (LDA here)\n",
    "\n",
    "Some common types of topic modeling for texts:\n",
    "\n",
    "1. TF-IDF, term frequency * inverse document frequency (TF-IDF). This is a weighting scheme to represent the importance of each word to identifying the document.\n",
    "    1. Fast\n",
    "    2. Easy to implement\n",
    "    3. Great for things like matching company names in Hilton San Diego, Hilton is more relevant than San Diego, especially if San Diego is lumped together with other location entities (\"GPE\" in Spacy), it gets more weight.\n",
    "2. Latent semantic indexing (LSI). Matrix decomposition method (at its core a SVD based deconstruction) to find the \"principle components\" of sparse matrix spaces, think of it as finding the span of the vectorized token space. The token vectors used as inputs are commonly TF-IDF representations.\n",
    "    1. Relatively fast, does not require maximizing likelihood functions, etc.\n",
    "    2. Reduces dimensions (vs. TF-IDF)\n",
    "    3. Has both positive and negative weights to define \"topics.\" Negative weights mean the topic is less likely if its associated token is in a document.\n",
    "    4. Great alternative to LDA.\n",
    "3. [Latent Dirichlet Allocation](http://ai.stanford.edu/~ang/papers/jair03-lda.pdf) (LDA). A generative hierarchical model of word distributions. Specifies a nested distributional structures where documents are distributions of topics which are in turn distributions over the (entire) vocabulary.\n",
    "    1. Slower, but Gensim has a multicore implementation that reduces runtime to hours (vs days/weeks)\n",
    "    2. Richer representation of topics\n",
    "    3. Probably the standard that one would compare newer methods (typically variants of LDA) to.\n",
    "\n",
    "\n",
    "The multicore version is a Godsend!\n",
    "![image.png](https://www.dropbox.com/s/ly2jzk303e613rn/Screenshot%202018-02-22%2013.50.18.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntopics = 30\n",
    "ldafile = 'lda_'+str(ntopics)\n",
    "if 1 ==1:\n",
    "    lda = tx._lda_(gensim_dictionary, corpus_path = grammed_corpus, numtopics= ntopics,iterations=100) # defaults to 10 topics\n",
    "    lda.save(tx.fpathroot+ldafile)\n",
    "else: \n",
    "    lda = LdaMulticore.load(tx.fpathroot+ldafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "ldaviz = pyLDAvis.gensim.prepare(lda, grammed_corpus, gensim_dictionary)\n",
    "pyLDAvis.save_html(ldaviz, '../Figures/viz_'+ldafile+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
